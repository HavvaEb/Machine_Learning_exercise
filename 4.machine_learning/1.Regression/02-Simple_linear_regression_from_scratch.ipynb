{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGRCdv2_63Fm"
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "Now that we've created our first learning machine model, let's see how it works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IHjwpyJmYz0"
   },
   "source": [
    "## How does it work?\n",
    "Here comes a part that some of you fear: Mathematics!    \n",
    "\n",
    "But don't worry, you'll see that it's not that complicated.\n",
    "\n",
    "In this notebook, everytime we will talk about programming variables, we will format the names like `this`. \n",
    "For mathematical variables and functions, we'll be formatting them like $this$.\n",
    "\n",
    "### How to calculate the y-axis from the x-axis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPLnFMBxs4C9"
   },
   "source": [
    "A linear model is in fact based on a simple [affine function.](https://fr.wikipedia.org/wiki/Fonction_affine) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XTFIZfapioo"
   },
   "source": [
    "$$f(x) = ax + b$$\n",
    "or ...\n",
    "$$y = f(x) = ax + b$$\n",
    "or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "y = a*x + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a function `f` which receives as a parameter the variables `x`,`a` and `b` and returns `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x, a, b):\n",
    "    y = a*x+b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will allow us to create a straight line that passes through all the points as well as possible. For the moment, we do not know the value of parameters $a$ and $b$, so it is impossible to draw a good straight line on the scatter plot, unless we choose parameters at random. And that is what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl6jA7JTdoO8"
   },
   "source": [
    "\n",
    "The linear model with random parameters would look something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Va6jerdvjC"
   },
   "source": [
    "![image.png](./assets/random_bias.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x4DbSIbeQNA"
   },
   "source": [
    "But we want to achieve this result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7hNlrnOezsy"
   },
   "source": [
    "![](./assets/trained_bias.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function: sum the errors (distance of sample point from the regression line) should be minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKo1rgh_fA7O"
   },
   "source": [
    "And it will be the role of the machine to learn how to find these values ($a$ and $b$) by minimizing the cost function that we will see in detail in the next chapter.\n",
    "\n",
    "Before we do that, we need to look at the small problem we have with this method. The function as written above only take one element, $x$. However, in the next sections, we are going to have multiple values for $x$, in a vector $X$. We'll thus denote a single item with an index $i$, like $x^{i}$. If we execute the function as is, we would have to make a loop for each element $x^{i}$ of our dataset. \n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}\\\\\n",
    "x^{(2)}\\\\\n",
    "x^{(3)}\\\\\n",
    "... \\\\\n",
    "x^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be very expensive in terms of machine resources. If your dataset is very large, it will take a lot of time to train your model. \n",
    "\n",
    "To solve this problem, it is customary to use something you are beginning to know, matrices! \n",
    "\n",
    "The matrices allow us to perform the function only once on our entire dataset. \n",
    "\n",
    "The matrix writing of $f(x)=ax+b$ is written like this:\n",
    "$$ F = X \\cdot \\theta$$\n",
    "\n",
    "As these are matrices that contain all the data, by convention, we put them in uppercase.\n",
    "\n",
    "The variable $F$ will contain a matrix with the set of predictions of $x^{(i)}$. \n",
    "\n",
    "$$ \n",
    "F \\\\\n",
    "\\begin{bmatrix}\n",
    "f(x^{(1)})\\\\\n",
    "f(x^{(2)})\\\\\n",
    "f(x^{(3)})\\\\\n",
    "... \\\\\n",
    "f(x^{(m)})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $\\theta$ (pronounced theta) will contain a vector with the values $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\\\\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $X$ will contain a matrix with two dimensions, one dimension with the value of $x^{(i)}$ and another dimension with 1's everywhere.  Why? Because we have to multiply our $X$ and $\\theta$ matrices. Remember [matrix multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html)?\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "x^{(1)} && 1\\\\\n",
    "x^{(2)} && 1\\\\\n",
    "x^{(3)} && 1\\\\\n",
    "... \\\\\n",
    "x^{(m)} && 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "![](./assets/dot_mat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which amounts to writing this for $y^{(1)}$ (and for every other $y^{(i)}$): \n",
    "$$ y^{(1)} = x^{(1)}* a + 1 * b$$\n",
    "\n",
    "And if we simplify:\n",
    "$$ y^{(1)} = ax^{(1)} + b$$\n",
    "\n",
    "\n",
    "So we are back to our original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :**\n",
    "1. Create a variable `X` which contains a matrix of shape `(30,2)` with a column filled with values of our dataframe (the same as the first notebook) and then another one with 1's. \n",
    "2. Create the `theta` variable which contains a vector with 2 random values.\n",
    "3. Create a variable `F` which contains a multiplication of the matrix `X` with the `theta` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "[1.1079226  1.16861096 1.22929932 1.38102023 1.44170859 1.65411786\n",
      " 1.68446204 1.74515041 1.74515041 1.89687131 1.95755968 1.98790386\n",
      " 1.98790386 2.01824804 2.13962477 2.26100149 2.32168986 2.38237822\n",
      " 2.56444331 2.59478749 2.83754094 2.92857348 3.17132694 3.26235948\n",
      " 3.41408039 3.50511293 3.65683384 3.68717802 3.89958729 3.96027566]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/salary_data.csv\")\n",
    "X1 = np.array(df.YearsExperience)\n",
    "X2 = np.ones(30)\n",
    "X=[X1,X2]\n",
    "X = np.vstack((X1, X2))\n",
    "X=X.T\n",
    "#print(X)\n",
    "print(X.shape)\n",
    "# X is a 30x2 matrix\n",
    "theta = np.random.random(2)\n",
    "F = np.matmul(X,theta)\n",
    "\n",
    "print(F)\n",
    "y_target= np.array(df.Salary)\n",
    "y=y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(30, 2)\n",
      "(2,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(theta.shape)\n",
    "\n",
    "x =  sum((np.dot(X , theta) - y))\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(X.size)\n",
    "print(y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a `model` function that receives as parameter `X` and `theta`.  The function must compute and return `F`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def model(X,theta):\n",
    "    #F = np.matmul(X,theta)\n",
    "    #return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda X, theta: X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a `y_pred` variable, call the `model` function with `X` and  `theta` and assign it to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to apply our model to our entire dataset. Now we have to know how to find the right values for $a$ and $b$. For that we will have to calculate the average of all our errors with a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKaQ28Hafn_h"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRBuUaU2ftck"
   },
   "source": [
    "The cost function allows us to evaluate the performance of our model by measuring the errors between the prediction and the actual value. The question we ask ourselves is: How to measure these errors?\n",
    "\n",
    "Imagine that you have 4 years of experience and that you earn $110000€$ per year. Your machine learning model predicts that this salary is worth $€90000$. You can conclude that your model therefore makes an error of $90000 - 110000 = -20000 €$.\n",
    "\n",
    "Thus, you could say that to measure your errors, you have to calculate the difference $f(x)-y$. However, if your prediction $f(x)$ is less than $y$, then this error is negative (as in the example above), and it is not very practical to minimize this function.\n",
    "\n",
    "So, to measure the errors between the $f(x)$ predictions and the target values $y$ of the dataset, we calculate the square of the difference: $(f(x)-y)^2$. This, by the way, is what is called the Euclidean norm, which represents the direct distance between $f(x)$ and $y$ in Euclidean geometry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJFTEvD8wb7b"
   },
   "source": [
    "\n",
    "\n",
    "![image.png](./assets/eucli.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa-c_X4xt2vO"
   },
   "source": [
    "But this is not enough. Indeed, we have the error of a single example. But we must have the average of all the errors of all the points. \n",
    "\n",
    "We could write it like this: \n",
    "\n",
    "\n",
    "\n",
    "$$MSE(a,b) = {\\dfrac{(f(x^{(1)})- y^{(1)})^2 + (f(x^{(2)})- y^{(2)})^2  + ... +(f(x^{(m)})- y^{(m)})^2}{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUWy24S5b27"
   },
   "source": [
    "Why $MSE$? Because this function is called **Mean Squared Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWIompHxm2Q"
   },
   "source": [
    "By convention this function is written in the following way, adding a coefficient $\\frac{1}{2}$ to simplify a derivative calculation that will come later.\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (f(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (ax^{(i)} +b - y^{(i)})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we work with matrices, we also have to transcribe our formula which becomes : \n",
    "\n",
    "$$MSE(\\theta) = \\frac {1}{2m}  \\sum (X \\cdot \\theta - Y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or something **similar** to this: \n",
    "\n",
    "```py\n",
    "MSE = 1/(2*m) * sum((X * theta - y)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47zckFxMQiVs"
   },
   "source": [
    "**Exercise :** Create a `MSE` function that receives as parameter `X, y and theta` using the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m= y.size # number of training examples\n",
    "def MSE(X,y,theta):\n",
    "    m=len(X)\n",
    "    mse=1/(2*m) * sum((np.dot(X , theta) - y)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3251273939.1406837"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(X, y,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_2cfr2DOB9X"
   },
   "source": [
    "### Minimize the cost function.\n",
    "\n",
    "\n",
    "\n",
    "If the cost function is omitted with respect to the parameter, it looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASRXsJyO__ic"
   },
   "source": [
    "![image.png](./assets/convexe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-cHhR_DBNI"
   },
   "source": [
    "The aim is therefore to reach the lowest point of the curve, i.e. the lowest possible sum of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqyzIhPKDMc_"
   },
   "source": [
    "![image.png](./assets/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, there are several function minimization algorithms, such as the least squares method or **gradient descent**. We will focus here on gradient descent because it is one of the most widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gmDtf6l-f7T"
   },
   "source": [
    "Gradient descent is an iterative algorithm which therefore proceeds by progressive improvements. For a linear problem, this algorithm needs to have two hyper-parameters :\n",
    "\n",
    "**1. The number of iterations :** As its name indicates, this is the parameter that will determine the number of iterations.\n",
    "\n",
    "**2. The learning rate :** This is the length of the step between each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMJ0V9J6HQZa"
   },
   "source": [
    "![learningrate](./assets/gradient_descent_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to clearly define the learning rate. If you set a high value, the algorithm will be faster, but you risk never reaching the lowest point of the curve, the steps being too big. Our model will never be able to work since it cannot find the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TASxiF6zHnq0"
   },
   "source": [
    "![](./assets/gradient_descent_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if you set a small value, then the algorithm will find the lowest point of the curve, but it will be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9on9fx_9HhYb"
   },
   "source": [
    "![learning rate](./assets/gradient_descent_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we will have to calculate the regression slope. \n",
    "![](./assets/derivative.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in mathematics we calculate a slope with a [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative#:~:text=In%20mathematics%2C%20a%20partial%20derivative,vector%20calculus%20and%20differential%20geometry.). The symbol used to denote partial derivatives is $\\partial$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac {\\partial MSE(\\theta) }{\\partial \\theta}  = \\frac {1}{m} X^T \\cdot (X \\cdot \\theta - Y)$$\n",
    "\n",
    "The  $^T$ in $X^T$ is to transpose the matrix, just like in numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could translate this into code like this:\n",
    "\n",
    "```py\n",
    "1/m * X.T.dot(model(X, theta) - y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    return 1/m * X.T.dot(model(X, theta) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-477383.6577915 ,  -76000.61357589])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1079226 , 1.16861096, 1.22929932, 1.38102023, 1.44170859,\n",
       "       1.65411786, 1.68446204, 1.74515041, 1.74515041, 1.89687131,\n",
       "       1.95755968, 1.98790386, 1.98790386, 2.01824804, 2.13962477,\n",
       "       2.26100149, 2.32168986, 2.38237822, 2.56444331, 2.59478749,\n",
       "       2.83754094, 2.92857348, 3.17132694, 3.26235948, 3.41408039,\n",
       "       3.50511293, 3.65683384, 3.68717802, 3.89958729, 3.96027566])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to write the gradient descent. \n",
    "\n",
    "$$\\theta = \\theta - \\eta *  \\frac {\\partial MSE(\\theta) }{\\partial \\theta}$$\n",
    "\n",
    "The variable $\\eta$ is the learning rate. So at each iteration, we redefine $\\theta$. We do : `theta` - `learning_rate` multiplied by the partial derivative of mean squared error. You could translate this into code like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "theta = theta - learning_rate * grad(X, y, theta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :**\n",
    "1. Create a `gradient_descent` function that receives as parameter `X`, `y`, `theta`, `learning_rate` and `n_iterations`.\n",
    "2. In the function, create a variable `cost_history` with a matrix filled with 0 and which has a length of `n_iterations`. We will use it to display the plot of the model learning process.\n",
    "3. Create a loop that iterates up to `n_iterations`.\n",
    "4. In the loop, update `theta` with the formula of the gradient descent (the example above).\n",
    "5. In the loop, update `cost_history[i]` with the values of `MSE(X,y,theta)`.\n",
    "6. return `theta` and `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    for i in range(n_iterations):\n",
    "        print(i)\n",
    "        theta = theta - learning_rate * grad(X, y, theta)\n",
    "        cost_history[i] = MSE(X,y,theta) # values of MSE\n",
    "        \n",
    "         #cost_history.append(computeCost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model\n",
    "\n",
    "Now that we know which algorithm is used to minimize the cost function, we train our model.   \n",
    "We define a number of iterations, and a learning step $\\alpha$, and here we go!\n",
    "\n",
    "Once the model is trained, we observe the results compared to our dataset.\n",
    "\n",
    "**Exercise :** Create variables `n_iterations` and `learning_rate`. \n",
    "The learning rate and the number of iterations are defined by trying around a little bit. You have to try several things, there is no magic number. However, starting with 1000 iterations and a learning rate of 0.01 is a good basis to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1= regressor.coef_\n",
    "#a2 = regressor.intercept_\n",
    "#y=a1*x + a2\n",
    "n_iterations = 1000\n",
    "learning_rate = 0.01 #alpha\n",
    "y=y_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `theta_final`, `cost history` and call `gradient_descent()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \n",
    "1. Create a `predictions` variable that will store the result of `model(X, theta_final)`.\n",
    "2. Use matplotlib to display a scatter plot with the data and the target.\n",
    "3. On the same graph, use the `plot` method to display your predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X, theta_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Be.code\\Machine_learning\\4.machine_learning\\1.Regression\\02-Simple_linear_regression_from_scratch.ipynb Cell 82'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Be.code/Machine_learning/4.machine_learning/1.Regression/02-Simple_linear_regression_from_scratch.ipynb#ch0000070?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mscatter(X1, y_target, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Be.code/Machine_learning/4.machine_learning/1.Regression/02-Simple_linear_regression_from_scratch.ipynb#ch0000070?line=5'>6</a>\u001b[0m \u001b[39m#plt.scatter(X1, y_target, data=predictions,color='r') \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Be.code/Machine_learning/4.machine_learning/1.Regression/02-Simple_linear_regression_from_scratch.ipynb#ch0000070?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(X1, y_target, data\u001b[39m=\u001b[39;49mpredictions, color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mred\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Be.code/Machine_learning/4.machine_learning/1.Regression/02-Simple_linear_regression_from_scratch.ipynb#ch0000070?line=8'>9</a>\u001b[0m \u001b[39m#plt.plot(predictions)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Be.code/Machine_learning/4.machine_learning/1.Regression/02-Simple_linear_regression_from_scratch.ipynb#ch0000070?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mScatter Plot\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/pyplot.py?line=2754'>2755</a>\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/pyplot.py?line=2755'>2756</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/pyplot.py?line=2756'>2757</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/pyplot.py?line=2757'>2758</a>\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/pyplot.py?line=2758'>2759</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1389'>1390</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1390'>1391</a>\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1391'>1392</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1628'>1629</a>\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1629'>1630</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1630'>1631</a>\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1631'>1632</a>\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1632'>1633</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_axes.py?line=1633'>1634</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:269\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=258'>259</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:  \u001b[39m# Can be x, y or y, c.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=259'>260</a>\u001b[0m     \u001b[39m# Figure out what the second argument is.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=260'>261</a>\u001b[0m     \u001b[39m# 1) If the second argument cannot be a format shorthand, the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=265'>266</a>\u001b[0m     \u001b[39m#    b) otherwise, it is indeed a format shorthand; use the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=266'>267</a>\u001b[0m     \u001b[39m#       first argument as label_namer.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=267'>268</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=268'>269</a>\u001b[0m         _process_plot_format(args[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=269'>270</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:  \u001b[39m# case 1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=270'>271</a>\u001b[0m         label_namer_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:170\u001b[0m, in \u001b[0;36m_process_plot_format\u001b[1;34m(fmt)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=167'>168</a>\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(fmt):\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=168'>169</a>\u001b[0m     c \u001b[39m=\u001b[39m fmt[i]\n\u001b[1;32m--> <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=169'>170</a>\u001b[0m     \u001b[39mif\u001b[39;00m fmt[i:i\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m] \u001b[39min\u001b[39;49;00m mlines\u001b[39m.\u001b[39;49mlineStyles:  \u001b[39m# First, the two-char styles.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=170'>171</a>\u001b[0m         \u001b[39mif\u001b[39;00m linestyle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=171'>172</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Mate%20Kovacs/AppData/Local/Programs/Python/Python310/lib/site-packages/matplotlib/axes/_base.py?line=172'>173</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mIllegal format string \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m; two linestyle symbols\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m fmt)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVKElEQVR4nO3dXYxcd3nH8e9ju6EsbfPiWBG1Y68lLKqA2kJGaVokVCUtcQrCuUBV0La4JapvoKWlEk3qC27qqqhVAwhIZRHAlFVC5NLGalVSN0HipgmsoQp5gWZLvI7dhGxxEqpaIg1+enH+i8frPbvreTvz8v1Iq5l55szMf1fy/Px/OycyE0mSVrKh6QZIkoaXISFJqmVISJJqGRKSpFqGhCSp1qamG9BrV155ZU5PTzfdDEkaKceOHfvvzNyyvD52ITE9Pc3c3FzTzZCkkRIRCyvVHW6SJNUyJCRJtQwJSVItQ0KSVMuQkCTVMiQkaQTMzsL0NGzYUN3Ozg7mcw0JSRpys7Owbx8sLEBmdbtv37mg6GeAjN0+CUkaN/v3w5kz59fOnKnqUAXG0vNLAQIwM9P9Z9uTkKQhd+JEfX2tAOmWISFJQ2779vr6agHSC4aEJA25Awdgaur82tRUVV8tQHphzZCIiM9ExPMR8Vhb7S8j4tsR8WhE/H1EXNb23B0RMR8R34mIm9rqu0ttPiJub6vvjIhHSv2LEXFJqb+qPJ4vz0/35leWpNEyMwMHD8KOHRBR3R48WNVXC5BeWE9P4nPA7mW1o8AbM/Pngf8A7gCIiGuAW4E3lNd8KiI2RsRG4JPAzcA1wLvLsQAfAe7MzNcBLwC3lfptwAulfmc5TpIm0swMHD8OZ89Wt0uT0qsFSC+sGRKZ+VXg9LLav2TmK+Xhw8C2cn8PcG9m/jAznwbmgevKz3xmfjczXwbuBfZERAA3AIfL6w8Bt7S916Fy/zBwYzlektSmLkB6oRdzEu8F/rnc3wo80/bcyVKrq28GXmwLnKX6ee9Vnn+pHH+BiNgXEXMRMbe4uNj1LyRJqnQVEhGxH3gFGNDev5Vl5sHMbGVma8uWC66ZIUnqUMeb6SLid4B3ADdmZpbyKeDqtsO2lRo19e8Dl0XEptJbaD9+6b1ORsQm4NJyvCRpQDrqSUTEbuBDwDszs30bxxHg1rIyaSewC/ga8HVgV1nJdAnV5PaREi5fAd5VXr8XuL/tvfaW++8CHmoLI0nSAKxnCew9wL8Br4+IkxFxG/AJ4KeBoxHx7xHxNwCZ+ThwH/AE8GXgfZn5o9JLeD/wAPAkcF85FuBPgA9GxDzVnMPdpX43sLnUPwj8eNmsJA2Lpk68Nygxbv85b7Va6TWuJQ3C0on32k+LMTXV2yWogxIRxzKztbzujmtJ6lC/z5s0DAwJSepQv8+bNAwMCUnqUL/PmzQMDAlJ6lC/z5s0DAwJSepQv8+bNAwMCUlDaVSWlvbzvEnDwMuXSho6y5eW9vqSnFo/exKShs4kLC0dFYaEpKEzqktLR2WI7GIYEpKGziguLV0aIltYgMxzQ2SjHhSGhKShM4pLS8d1iMyQkDR0RnFp6agOka3F1U2ShtLMzHCHwnLbt1dDTCvVR5k9CUnqgVEcIlsPQ0KSemAUh8jWw+EmSeqRURsiWw97EpLG0jjuWWiCPQlJY8fTevSOPQlJY2dc9yw0wZCQNHbGdc9CEwwJSWNnFE/rMawMCUljZ1z3LDTBkJA0dsZ1z0ITXN0kaSyN456FJtiTkDTR3E+xOnsSkiaW+ynWZk9C0sRyP8XaDAlJE8v9FGszJCRNLPdTrM2QkDSx3E+xNkNC0sRyP8XaXN0kaaK5n2J19iQkSbUMCUlSLUNC0kC5w3m0OCchaWDc4Tx67ElIGhh3OI+eNUMiIj4TEc9HxGNttSsi4mhEPFVuLy/1iIiPR8R8RDwaEW9ue83ecvxTEbG3rX5tRHyrvObjERGrfYak0eUO59Gznp7E54Ddy2q3Aw9m5i7gwfIY4GZgV/nZB9wF1Rc+8GHgl4DrgA+3fenfBfxe2+t2r/EZkkaUO5xHz5ohkZlfBU4vK+8BDpX7h4Bb2uqfz8rDwGUR8VrgJuBoZp7OzBeAo8Du8tzPZObDmZnA55e910qfIWlEucN59HQ6J3FVZj5b7j8HXFXubwWeaTvuZKmtVj+5Qn21z5A0otzhPHq6Xt2UmRkR2YvGdPoZEbGPaniL7fZbpaHmDufR0mlP4ntlqIhy+3ypnwKubjtuW6mtVt+2Qn21z7hAZh7MzFZmtrZs2dLhryRNNvcvaCWdhsQRYGmF0l7g/rb6e8oqp+uBl8qQ0QPA2yLi8jJh/TbggfLcDyLi+rKq6T3L3mulz5DUY0v7FxYWIPPc/gWDQlHNF69yQMQ9wK8CVwLfo1ql9A/AfcB2YAH4zcw8Xb7oP0G1QukM8LuZOVfe573An5a3PZCZny31FtUKqlcD/wz8fhle2rzSZ6z1C7VarZybm1vnry8Jqp7DwsKF9R074PjxQbdGTYiIY5nZuqC+VkiMGkNCungbNlQ9iOUi4OzZwbdHg1cXEu64luT+BdUyJCS5f0G1DAlJ7l9QLc8CKwlw/4JWZk9CklTLkJAk1TIkpAa5y1nDzjkJqSFepU2jwJ6E1BCv0qZRYEhIDfEqbRoFhoTUEHc5axQYElJDBrnL2QlydcqQkBoyqF3OngZc3fAssNKY8zTgWg/PAitNKCfI1Q1DQhpzTpCrG4aENOY8Dbi6YUhIY87TgKsbnpZDmgCeBlydsichSaplSEiSahkSkqRahoQkqZYhIUmqZUhIkmoZEpKkWoaEJKmWISFJqmVISJJqGRKSpFqGhCSpliEhrcHrQ2uSeRZYaRVL14c+c6Z6vHR9aPCsqpoM9iSkVezffy4glpw5U9WlSWBISKvw+tCadIaEtIp+XB/aOQ6NEkNCWkWvrw+9NMexsACZ5+Y4DAoNK0NCWkWvrw/tHIdGTVchERF/FBGPR8RjEXFPRPxkROyMiEciYj4ivhgRl5RjX1Uez5fnp9ve545S/05E3NRW311q8xFxezdtlTo1MwPHj8PZs9VtN6uanOPQqOk4JCJiK/AHQCsz3whsBG4FPgLcmZmvA14AbisvuQ14odTvLMcREdeU170B2A18KiI2RsRG4JPAzcA1wLvLsdLI6scch9RP3Q43bQJeHRGbgCngWeAG4HB5/hBwS7m/pzymPH9jRESp35uZP8zMp4F54LryM5+Z383Ml4F7y7HSyOr1HIfUbx2HRGaeAv4KOEEVDi8Bx4AXM/OVcthJYGu5vxV4prz2lXL85vb6stfU1S8QEfsiYi4i5hYXFzv9laS+6/Uch9Rv3Qw3XU71P/udwM8Cr6EaLhq4zDyYma3MbG3ZsqWJJkjr1ss5Dqnfuhlu+jXg6cxczMz/A74EvAW4rAw/AWwDTpX7p4CrAcrzlwLfb68ve01dXZI0IN2ExAng+oiYKnMLNwJPAF8B3lWO2QvcX+4fKY8pzz+UmVnqt5bVTzuBXcDXgK8Du8pqqUuoJrePdNFeSdJF6vgEf5n5SEQcBr4BvAJ8EzgI/BNwb0T8WandXV5yN/C3ETEPnKb60iczH4+I+6gC5hXgfZn5I4CIeD/wANXKqc9k5uOdtleSdPGi+s/8+Gi1Wjk3N9d0MyRppETEscxsLa+741qSVMuQkCTVMiQkSbUMCUlSLUNCklTLkJAk1TIkJEm1DAlJUi1DQpJUy5CQJNUyJCRJtQwJSVItQ0KSVMuQkCTVMiQkSbUMCUlSLUNCklTLkJAk1TIkJEm1DAlNlNlZmJ6GDRuq29nZplskDbdNTTdAGpTZWdi3D86cqR4vLFSPAWZmmmuXNMzsSWhi7N9/LiCWnDlT1SWtzJDQxDhx4uLqkgwJTZDt2y+uLsmQ0AQ5cACmps6vTU1VdXBSW1qJIaGJMTMDBw/Cjh0QUd0ePFjVlya1FxYg89yktkGhSReZ2XQbeqrVauXc3FzTzdCImZ6ugmG5HTvg+PFBt0YavIg4lpmt5XV7EhJOakt1DAldYBLH5p3UllZmSOg8kzo2v9aktjSpDAmdp98bzoa1l7LapLY0yZy41nk2bKh6EMtFwNmz3b338tNiQPW/db+MpeY5ca116efYvKfFkEaPIaHz9HNs3hVE0ugxJHSefo7Nu4JIGj2GhC4wM1NtIDt7trrt1XxBr3spwzoJLo0TQ0ID08teyqQu1ZUGzdVNGkmeRkPqrb6sboqIyyLicER8OyKejIhfjogrIuJoRDxVbi8vx0ZEfDwi5iPi0Yh4c9v77C3HPxURe9vq10bEt8prPh4R0U17db5RHq5xElwajG6Hmz4GfDkzfw74BeBJ4HbgwczcBTxYHgPcDOwqP/uAuwAi4grgw8AvAdcBH14KlnLM77W9bneX7VUx6sM1ToJLg9FxSETEpcBbgbsBMvPlzHwR2AMcKocdAm4p9/cAn8/Kw8BlEfFa4CbgaGaezswXgKPA7vLcz2Tmw1mNiX2+7b3UpVHfs+BpNKTB6KYnsRNYBD4bEd+MiE9HxGuAqzLz2XLMc8BV5f5W4Jm2158stdXqJ1eoXyAi9kXEXETMLS4udvErTY5RH67xNBrSYHQTEpuANwN3ZeabgP/l3NASAKUH0PeZ8cw8mJmtzGxt2bKl3x83FsZhuKZfS3UlndNNSJwETmbmI+XxYarQ+F4ZKqLcPl+ePwVc3fb6baW2Wn3bCnX1gMM1ktaj45DIzOeAZyLi9aV0I/AEcARYWqG0F7i/3D8CvKescroeeKkMSz0AvC0iLi8T1m8DHijP/SAiri+rmt7T9l7qUj+Ha0Z51ZSkZTKz4x/gF4E54FHgH4DLgc1Uq5qeAv4VuKIcG8Angf8EvgW02t7nvcB8+fndtnoLeKy85hOUfR2r/Vx77bWp5nzhC5lTU5nVmqnqZ2qqqq907I4dmRGZmzdXPxFVbaXjJfUPMJcrfKe6mW6CzM5Wq5dOnKjmHg4c6P04/no3ua102vB2nkJcGqy6zXSGxIQY1LUc1ns9irowaefuaWlwvJ7EhBvUvoj1rppaz1LbUVmOK40zQ2JCDGpfxHpXTV1xxdrvNUrLcaVxZUhMiEHti+jVqimX40rDwZCYEBezL6LbJazr2eR2+nT96909LQ2PTU03QIOx9IW71uqm5RPcSyf+a3+PXti+3VN9S6PA1U06z6Cu0zCo1VaS1sfVTVqXQU1we4I+aTQ43KTz1A0D9WOl0cyMoSANO3sSOo8n/pPUzpDQeRwGktTO4SZdwGEgSUvsSUiSahkSkqRahoQkqZYhIUmqZUhIkmoZEpKkWoaEJKmWISFJqmVISJJqGRKSpFqGhCSpliGxhm4v5SlJo8wT/K1iUJfylKRhZU9iFfv3n395Tage79/fTHskadAMiVUM6lKekjSsDIlV1F2ysx+X8pSkYWRIrMJLeUqadIbEKvp5KU9XTUkaBa5uWkM/LuXpqilJo8KeRANcNSVpVBgSDXDVlKRRYUg0wFVTkkaFIdEAV01JGhWGRAP6uWpKknrJ1U0N6ceqKUnqta57EhGxMSK+GRH/WB7vjIhHImI+Ir4YEZeU+qvK4/ny/HTbe9xR6t+JiJva6rtLbT4ibu+2rZKki9OL4aYPAE+2Pf4IcGdmvg54Abit1G8DXij1O8txRMQ1wK3AG4DdwKdK8GwEPgncDFwDvLscO/bcaCdpWHQVEhGxDXg78OnyOIAbgMPlkEPALeX+nvKY8vyN5fg9wL2Z+cPMfBqYB64rP/OZ+d3MfBm4txzbc8P0pby00W5hATLPbbQzKCQ1oduexEeBDwFny+PNwIuZ+Up5fBLYWu5vBZ4BKM+/VI7/cX3Za+rqPTVsX8putJM0TDoOiYh4B/B8Zh7rYXs6bcu+iJiLiLnFxcWLeu2wfSm70U7SMOmmJ/EW4J0RcZxqKOgG4GPAZRGxtGpqG3Cq3D8FXA1Qnr8U+H57fdlr6uoXyMyDmdnKzNaWLVsu6pcYti9lN9pJGiYdh0Rm3pGZ2zJzmmri+aHMnAG+AryrHLYXuL/cP1IeU55/KDOz1G8tq592AruArwFfB3aV1VKXlM840ml763T6pdyveQw32kkaJv3YTPcnwAcjYp5qzuHuUr8b2FzqHwRuB8jMx4H7gCeALwPvy8wflXmL9wMPUK2euq8c21OdfCn3cx7DjXaShklU/5kfH61WK+fm5i7qNbOz1RzEiRNVD+LAgdW/lKenq2BYbscOOH78oj5akoZCRBzLzNYFdUPi4m3YUPUglouAs2cvrEvSsKsLCc/d1AEnlyVNCkOiA04uS5oUhkQHnFyWNCk8C2yHPIurpElgT0KSVMuQkCTVMiQkSbUMCUlSLUNCklRr7HZcR8QisMJJM4bWlcB/N92Ihvk38G8A/g2a/v13ZOYFp9Eeu5AYNRExt9JW+Eni38C/Afg3GNbf3+EmSVItQ0KSVMuQaN7BphswBPwb+DcA/wZD+fs7JyFJqmVPQpJUy5CQJNUyJBoSEVdHxFci4omIeDwiPtB0m5oQERsj4psR8Y9Nt6UJEXFZRByOiG9HxJMR8ctNt2nQIuKPyr+BxyLinoj4yabb1G8R8ZmIeD4iHmurXRERRyPiqXJ7eZNtXGJINOcV4I8z8xrgeuB9EXFNw21qwgeAJ5tuRIM+Bnw5M38O+AUm7G8REVuBPwBamflGYCNwa7OtGojPAbuX1W4HHszMXcCD5XHjDImGZOazmfmNcv9/qL4ctjbbqsGKiG3A24FPN92WJkTEpcBbgbsBMvPlzHyx0UY1YxPw6ojYBEwB/9Vwe/ouM78KnF5W3gMcKvcPAbcMsk11DIkhEBHTwJuARxpuyqB9FPgQcLbhdjRlJ7AIfLYMuX06Il7TdKMGKTNPAX8FnACeBV7KzH9ptlWNuSozny33nwOuarIxSwyJhkXETwF/B/xhZv6g6fYMSkS8A3g+M4813ZYGbQLeDNyVmW8C/pchGWIYlDLuvocqMH8WeE1E/FazrWpeVnsThmJ/giHRoIj4CaqAmM3MLzXdngF7C/DOiDgO3AvcEBFfaLZJA3cSOJmZSz3Iw1ShMUl+DXg6Mxcz8/+ALwG/0nCbmvK9iHgtQLl9vuH2AIZEYyIiqMain8zMv266PYOWmXdk5rbMnKaaqHwoMyfqf5CZ+RzwTES8vpRuBJ5osElNOAFcHxFT5d/EjUzY5H2bI8Decn8vcH+DbfkxQ6I5bwF+m+p/0P9efn6j6UZp4H4fmI2IR4FfBP682eYMVulFHQa+AXyL6jtpKE9P0UsRcQ/wb8DrI+JkRNwG/AXw6xHxFFUP6y+abOMST8shSaplT0KSVMuQkCTVMiQkSbUMCUlSLUNCklTLkJAk1TIkJEm1/h8HMnhbszGRRQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.gca()\n",
    "X1 = np.array(df.YearsExperience)\n",
    "y_target= np.array(df.Salary)\n",
    "plt.scatter(X1, y_target, color=\"blue\")\n",
    "#plt.scatter(X1, y_target, data=predictions,color='r') \n",
    "\n",
    "plt.plot(X1, y_target, data=predictions, color='red')\n",
    "#plt.plot(predictions)\n",
    "plt.title(\"Scatter Plot\")\n",
    "plt.xlabel(\"Years of Exp.\")\n",
    "plt.ylabel(\"Salary of employee\")\n",
    "plt.show()\n",
    "#plt.scatter(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this. \n",
    "![](./assets/final_theta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, change the learning rate and the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "To check if our gradient descent algorithm worked well, we observe the evolution of the cost function through iterations. We are supposed to obtain a curve that decreases with each iteration until it stagnates at a minimal level (close to zero). If the curve does not follow this pattern, then the learning rate may be too high, we should take a smaller step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \n",
    "1. Plot the `cost_history` with respect to the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "plt.title('cost_history')\n",
    "plt.ylabel('Theta')\n",
    "plt.xlabel('Iterations')\n",
    "_=ax.plot(range(n_iterations),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this: \n",
    "![](./assets/learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, we can see that after 400 iterations, the model no longer learns and becomes constant. We can thus redefine the maximum number of iterations to 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the real performance of our model with a popular metric (for your boss, client, or colleagues) we can use the coefficient of determination, also known as $R^2$. It comes from the method of least squares. The closer the result is to 1, the better your model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_target\n",
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred)**2).sum()\n",
    "    v = ((y - y.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9545028584887498"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_determination(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end\n",
    "Ok ok, you just built your own model of linear regression, do you realize that? \n",
    "This part was a bit theoretical, but it's essential to understand how it works.  \n",
    "![tired.gif](./assets/tired.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de linearregression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
